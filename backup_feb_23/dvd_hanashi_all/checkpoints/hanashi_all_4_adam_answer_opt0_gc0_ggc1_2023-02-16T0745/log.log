02/16/2023 07:45:35 0
02/16/2023 07:45:35 Launching the MT-DNN training
02/16/2023 07:45:35 Loading ../../../dvd_hanashi/dvd_hanashi_all/4/ppf_train.json as task 0
02/16/2023 07:45:35 Loading ../../../dvd_hanashi/dvd_hanashi_all/4/hanashiduration_train.json as task 1
02/16/2023 07:45:35 Loading ../../../dvd_hanashi/dvd_hanashi_all/4/genjitsukasou_train.json as task 2
02/16/2023 07:45:35 4,8,2
02/16/2023 07:45:35 ####################
02/16/2023 07:45:35 {'log_file': 'checkpoints/hanashi_all_4_adam_answer_opt0_gc0_ggc1_2023-02-16T0745/log.log', 'tensorboard': False, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': '../../../japanese_bert_new_/jap_bert.pt', 'data_dir': '../../../dvd_hanashi/dvd_hanashi_all/4/', 'data_sort_on': False, 'name': 'farmer', 'task_def': '../../../experiments/dvd_hanashi/dvd_hanashi_task_def.yml', 'train_datasets': ['ppf', 'hanashiduration', 'genjitsukasou'], 'test_datasets': ['ppf', 'hanashiduration', 'genjitsukasou'], 'update_bert_opt': 0, 'multi_gpu_on': True, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': [0, 0, 0], 'label_size': '4,8,2', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'encoder_type': <EncoderModelType.BERT: 1>, 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 10, 'batch_size': 16, 'batch_size_eval': 8, 'optimizer': 'adam', 'grad_clipping': 0.0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 1e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'ema_opt': 0, 'ema_gamma': 0.995, 'lookahead': False, 'lookahead_k': 5, 'lookahead_alpha': 0.5, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': 'checkpoints/hanashi_all_4_adam_answer_opt0_gc0_ggc1_2023-02-16T0745', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': False, 'fp16_opt_level': 'O1', 'mean_teacher': False, 'mean_teacher_avg': 'exponential', 'mean_teacher_lambda': 1, 'mean_teacher_rampup': 4000, 'mean_teacher_alpha1': 0.99, 'mean_teacher_alpha2': 0.999, 'mt_kd_tau': 1, 'virtual_teacher': False, 'vat_eps': 0.001, 'vat_max_layer': -1, 'vat_opt': 0, 'vat_lambda': 1, 'vat_alpha': 1, 'vat_nosiy': 1e-05, 'use_noisycopy': False, 'noisycopy_eps': 0.01, 'use_advcopy': False, 'advcopy_eps': 0.01, 'vat_stable_eps': 1e-06, 'tasks_dropout_p': [0.1, 0.1, 0.1]}
02/16/2023 07:45:35 ####################
02/16/2023 07:45:35 ############# Gradient Accumulation Info #############
02/16/2023 07:45:35 number of step: 6660
02/16/2023 07:45:35 number of grad grad_accumulation step: 1
02/16/2023 07:45:35 adjusted number of step: 6660
02/16/2023 07:45:35 ############# Gradient Accumulation Info #############
02/16/2023 07:45:43 
############# Model Arch of MT-DNN #############
SANBertNetwork(
  (dropout_list): ModuleList(
    (0): DropoutWrapper()
    (1): DropoutWrapper()
    (2): DropoutWrapper()
  )
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(32006, 768)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (scoring_list): ModuleList(
    (0): Linear(in_features=768, out_features=4, bias=True)
    (1): Linear(in_features=768, out_features=8, bias=True)
    (2): Linear(in_features=768, out_features=2, bias=True)
  )
)

02/16/2023 07:45:43 Total number of params: 110632718
02/16/2023 07:45:43 At epoch 0
02/16/2023 07:45:43 Task [ 0] updates[     1] train loss[1.39477] remaining[0:07:02]
02/16/2023 07:47:57 Task [ 0] updates[   500] train loss[1.07906] remaining[0:00:44]
02/16/2023 07:48:44 Task ppf -- epoch 0 -- Dev ACC: 59.687
02/16/2023 07:48:44 Task ppf -- epoch 0 -- Dev F1: 31.577
02/16/2023 07:48:46 Task hanashiduration -- epoch 0 -- Dev ACC: 28.490
02/16/2023 07:48:46 Task hanashiduration -- epoch 0 -- Dev F1: 8.849
02/16/2023 07:48:49 Task genjitsukasou -- epoch 0 -- Dev ACC: 90.313
02/16/2023 07:48:49 Task genjitsukasou -- epoch 0 -- Dev F1: 47.455
02/16/2023 07:48:49 At epoch 1
02/16/2023 07:50:17 Task [ 1] updates[  1000] train loss[0.98377] remaining[0:01:28]
02/16/2023 07:51:50 Task ppf -- epoch 1 -- Dev ACC: 58.832
02/16/2023 07:51:50 Task ppf -- epoch 1 -- Dev F1: 40.799
02/16/2023 07:51:52 Task hanashiduration -- epoch 1 -- Dev ACC: 31.197
02/16/2023 07:51:52 Task hanashiduration -- epoch 1 -- Dev F1: 9.993
02/16/2023 07:51:55 Task genjitsukasou -- epoch 1 -- Dev ACC: 89.031
02/16/2023 07:51:55 Task genjitsukasou -- epoch 1 -- Dev F1: 51.787
02/16/2023 07:51:55 At epoch 2
02/16/2023 07:52:40 Task [ 1] updates[  1500] train loss[0.92105] remaining[0:02:13]
02/16/2023 07:54:54 Task ppf -- epoch 2 -- Dev ACC: 60.969
02/16/2023 07:54:54 Task ppf -- epoch 2 -- Dev F1: 43.947
02/16/2023 07:54:57 Task hanashiduration -- epoch 2 -- Dev ACC: 32.194
02/16/2023 07:54:57 Task hanashiduration -- epoch 2 -- Dev F1: 13.326
02/16/2023 07:54:59 Task genjitsukasou -- epoch 2 -- Dev ACC: 87.607
02/16/2023 07:54:59 Task genjitsukasou -- epoch 2 -- Dev F1: 54.423
02/16/2023 07:54:59 At epoch 3
02/16/2023 07:55:00 Task [ 1] updates[  2000] train loss[0.85604] remaining[0:03:32]
02/16/2023 07:57:14 Task [ 2] updates[  2500] train loss[0.79547] remaining[0:00:43]
02/16/2023 07:58:00 Task ppf -- epoch 3 -- Dev ACC: 62.108
02/16/2023 07:58:00 Task ppf -- epoch 3 -- Dev F1: 41.427
02/16/2023 07:58:03 Task hanashiduration -- epoch 3 -- Dev ACC: 30.912
02/16/2023 07:58:03 Task hanashiduration -- epoch 3 -- Dev F1: 14.529
02/16/2023 07:58:05 Task genjitsukasou -- epoch 3 -- Dev ACC: 88.034
02/16/2023 07:58:05 Task genjitsukasou -- epoch 3 -- Dev F1: 52.113
02/16/2023 07:58:05 At epoch 4
02/16/2023 07:59:35 Task [ 0] updates[  3000] train loss[0.75201] remaining[0:01:27]
02/16/2023 08:01:06 Task ppf -- epoch 4 -- Dev ACC: 62.251
02/16/2023 08:01:06 Task ppf -- epoch 4 -- Dev F1: 43.224
02/16/2023 08:01:08 Task hanashiduration -- epoch 4 -- Dev ACC: 29.915
02/16/2023 08:01:08 Task hanashiduration -- epoch 4 -- Dev F1: 13.854
02/16/2023 08:01:11 Task genjitsukasou -- epoch 4 -- Dev ACC: 89.031
02/16/2023 08:01:11 Task genjitsukasou -- epoch 4 -- Dev F1: 52.824
02/16/2023 08:01:11 At epoch 5
02/16/2023 08:01:56 Task [ 2] updates[  3500] train loss[0.70882] remaining[0:02:11]
02/16/2023 08:04:11 Task ppf -- epoch 5 -- Dev ACC: 60.684
02/16/2023 08:04:11 Task ppf -- epoch 5 -- Dev F1: 44.898
02/16/2023 08:04:13 Task hanashiduration -- epoch 5 -- Dev ACC: 28.632
02/16/2023 08:04:13 Task hanashiduration -- epoch 5 -- Dev F1: 15.902
02/16/2023 08:04:15 Task genjitsukasou -- epoch 5 -- Dev ACC: 87.607
02/16/2023 08:04:15 Task genjitsukasou -- epoch 5 -- Dev F1: 51.826
02/16/2023 08:04:16 At epoch 6
02/16/2023 08:04:17 Task [ 1] updates[  4000] train loss[0.67128] remaining[0:02:35]
02/16/2023 08:06:32 Task [ 1] updates[  4500] train loss[0.63674] remaining[0:00:43]
02/16/2023 08:07:18 Task ppf -- epoch 6 -- Dev ACC: 60.256
02/16/2023 08:07:18 Task ppf -- epoch 6 -- Dev F1: 43.802
02/16/2023 08:07:20 Task hanashiduration -- epoch 6 -- Dev ACC: 30.057
02/16/2023 08:07:20 Task hanashiduration -- epoch 6 -- Dev F1: 16.779
02/16/2023 08:07:23 Task genjitsukasou -- epoch 6 -- Dev ACC: 88.177
02/16/2023 08:07:23 Task genjitsukasou -- epoch 6 -- Dev F1: 51.235
02/16/2023 08:07:23 At epoch 7
02/16/2023 08:08:53 Task [ 2] updates[  5000] train loss[0.60858] remaining[0:01:27]
02/16/2023 08:10:22 Task ppf -- epoch 7 -- Dev ACC: 62.678
02/16/2023 08:10:22 Task ppf -- epoch 7 -- Dev F1: 46.087
02/16/2023 08:10:25 Task hanashiduration -- epoch 7 -- Dev ACC: 29.772
02/16/2023 08:10:25 Task hanashiduration -- epoch 7 -- Dev F1: 13.327
02/16/2023 08:10:27 Task genjitsukasou -- epoch 7 -- Dev ACC: 88.604
02/16/2023 08:10:27 Task genjitsukasou -- epoch 7 -- Dev F1: 52.511
02/16/2023 08:10:27 At epoch 8
02/16/2023 08:11:13 Task [ 1] updates[  5500] train loss[0.58119] remaining[0:02:12]
02/16/2023 08:13:26 Task ppf -- epoch 8 -- Dev ACC: 61.254
02/16/2023 08:13:26 Task ppf -- epoch 8 -- Dev F1: 44.722
02/16/2023 08:13:28 Task hanashiduration -- epoch 8 -- Dev ACC: 28.063
02/16/2023 08:13:28 Task hanashiduration -- epoch 8 -- Dev F1: 16.377
02/16/2023 08:13:31 Task genjitsukasou -- epoch 8 -- Dev ACC: 87.607
02/16/2023 08:13:31 Task genjitsukasou -- epoch 8 -- Dev F1: 52.727
02/16/2023 08:13:31 At epoch 9
02/16/2023 08:13:33 Task [ 2] updates[  6000] train loss[0.55826] remaining[0:03:11]
02/16/2023 08:15:46 Task [ 2] updates[  6500] train loss[0.53684] remaining[0:00:42]
02/16/2023 08:16:31 Task ppf -- epoch 9 -- Dev ACC: 62.393
02/16/2023 08:16:31 Task ppf -- epoch 9 -- Dev F1: 45.620
02/16/2023 08:16:34 Task hanashiduration -- epoch 9 -- Dev ACC: 27.208
02/16/2023 08:16:34 Task hanashiduration -- epoch 9 -- Dev F1: 15.086
02/16/2023 08:16:36 Task genjitsukasou -- epoch 9 -- Dev ACC: 88.462
02/16/2023 08:16:36 Task genjitsukasou -- epoch 9 -- Dev F1: 52.410
